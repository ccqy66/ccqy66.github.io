<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on ccqy66</title><link>https://ccqy66.com/</link><description>Recent content in Home on ccqy66</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 20 Jun 2024 11:13:21 +0800</lastBuildDate><atom:link href="https://ccqy66.com/index.xml" rel="self" type="application/rss+xml"/><item><title>浅谈神经网络</title><link>https://ccqy66.com/posts/%E6%B5%85%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Thu, 20 Jun 2024 11:13:21 +0800</pubDate><guid>https://ccqy66.com/posts/%E6%B5%85%E8%B0%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>随着2020年ChatGPT3的发布，人工智能迎来了快速的发展，此后，国内外各种大模型如雨后春笋般快速发展。
当然，本文将不会探究大模型的过多技术细节，了解「神经网络」能够更好地让我们理解大模型，所以本文仅仅从入门的角度上，谈谈作为人工智能底层技术的「神经网络」是什么？为什么可以有这么大的能量？
世界的本质-函数 这个问题有点偏哲学，但是放在今天讨论的话题上来看，再合适不过了。先说结论，在人工智能的世界里，我们可以说世界本质是函数。
示例1：当一个物体从高处做自由落体时，下落的高度$h$可以是关于$t$的函数，我们知道可以表示为:
$$h = \frac{1}{2}gt^2$$
示例2：一个浸在流体中的物体所受到竖直向上的浮力，其大小等于物体所排开流体的重力，用公式可以表示为: $$F_{浮力}=ρ_{液}gv_{排开流体}$$
通过这两个例子，我们发现，人类对这个世界的理解，就是通过函数的方式来描述世界的规律。基于这个结论，我们大胆假设，如果世间万物都有一个与时间$t$的函数关系，是不是我们就可以预测未来呢？
既然真实世界，我们可以将规律抽象成一个函数来描述，那么用计算机来解决问题是不是也可以定义为如何抽象成一个函数呢？
最小二乘法问题 在高中时，我们曾经学习过最小二乘法，来解决线性回归问题，线性回归问题也可以称为一种预测算法，通过历史数据来预测未知数据。
问题描述为：假设有一组数据，我们希望求出对应的一元线性模型来拟合这一组数据的映射关系： $$y = β_{0} + β_{1}x$$
同时，我们也知道，$β_{0}$和$β_{1}$对应的求解公式(有兴趣的话，可以看下推导过程：最小二乘法的推导证明：
$$β_{0}=\bar{y}-β_{1}\bar{x}$$ $$β_{1}=\frac{\sum_{i=1}^m(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^m(x_i-\bar{x})^2}$$
假如有这么一组数据：(25, 110), (27, 115), (31, 155), (33, 160), (35, 180)，请根据这些数据，预测当x=40时，对应的y值：
通过上面的公式，我们推导出，线性方程为：$y = 7.2x - 73.7$
于是，我们可以预测当x=40时，$y=214.3$
恭喜你，读到这里，其实你已经学会了神经网络的本质，能够通过现有的数据拟合一个函数，然后通过拟合的函数预测数据。而计算$β_{0}$和$β_{1}$的过程就是训练过程。
但是现实世界中的问题是很复杂的，并不总是线性的，而且我们也没有办法确定一个函数关系，例如是线性的，还是二次？所以我们需要一个通用的算法，能够根据我们提供的数据自动拟合一个函数关系。如果存在这种能力，我们就可以描述这个世界的一些规律。为了解决这个问题，提出了神经网络的理论。下面我们将分析神经网络是如何来解决我们的问题。
神经网络 神经网络是一种机器学习模型，它以类似于人脑的方式做出决策，通过使用模仿生物神经元协同工作方式的过程来识别现象、权衡利弊并得出结论。
每个神经网络都由多个节点层或人工神经元组成 – 一个输入层、一个或多个隐藏层和一个输出层。每个节点都与其他节点相连，具有一个关联的权重和阈值。如果任何单个节点的输出高于指定的阈值，那么该节点将被激活，并将数据发送到网络的下一层。否则，不会将数据传递到网络的下一层。 如上图所示，最左侧的为输入层，中间的三个为隐藏层，最后的为输出层。
神经元 上图为人的神经元的基本构造，通常一个神经元具有多个树突，主要用来接受传入信息信息，信息通过轴突传递进来后经过一系列的计算（细胞核）最终产生一个信号传递到轴突，轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。
提出神经网络之初，就是为了模拟人为的思考方式，所以基于人的神经元模型，人为也构造了类似的“人工神经元”模型。 一个神经元可以接受多个输入，并且每个输入都对应一个权重，这个权重是模型在训练所得。在神经元内部进行加和操作，在输出之前会通过一个激活函数进行非线性变化，具体为什么需要激活函数进行非线性变化，后面我们再分析。
通过上图，我们可以通过数学公式来描述一个神经元的计算逻辑： $$Z=g(\sum_{i=0}^{n}(x_{i}*w_{i})+b)$$
激活函数 从上一章节，我们发现，在神经元内部进行加权求和之后，又会经过一个激活函数处理，这个激活函数有什么作用呢？
我们假设，没有这个激活函数，并且这个神经网络有三层：输入层、隐藏层、输出层。 那么我们就知道，输入层到隐藏层的结果为:
$$Z_1 = \sum_{i=0}^{n}(x_{i}*w_{i})+b$$
换成向量表示： $$Z_1 = X*W_1^T+b$$
同理我们可以计算出，隐藏层到输出层的结果为: $$Z_2 = Z_1*W_2^T+b$$
因为$Z_1$和$Z_2$都是线性方程，所以本质上是可以用一个现场方程来表示的，也就是说这个神经网络不管有几层都没有意义，因为都能通过一层来表示。
所以为了解决这个问题，我们就得在每一层后面加一个非线性变换，这样，相邻的两层无法等价于一层。</description></item></channel></rss>